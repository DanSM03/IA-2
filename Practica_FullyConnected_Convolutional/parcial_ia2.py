# -*- coding: utf-8 -*-
"""PARCIAL_IA2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wt9puwszFdh_NSMmdjJvhw5_eLqLenVr

## **SECCIÓN 1**

**(1) Explique en que consiste la inteligencia artificial y como esta se relaciona con la
inteligencia humana. Incluya conceptos, teorías psicológicas y demás que
considere necesarias.**

Como se menciona en el acta del proyecto "A proposal for the Dartmonuth summer Research on Artificial Intelligence", la inteligencia artificial se refiere a la capacidad que se les otorga a objetos no biológicos de realizar tareas que normalmente solo realizaban humanos. Por otro lado si nos apegamos a la definición de Inteligencia artifical disponible en la RAE nos damos cuenta que esta se define como "Disciplina científica que se ocupa de crear programas informáticos que ejecutan operaciones comparables a las que realiza la mente humana, como el aprendizaje o el razonamiento lógico" y la comparamos con definiciones de inteligencia como las de "Ch. Spearman" quién decia que la inteligencia es la "Capacidad unitaria para resolver problemas y crear nuevos contenidos" y la de "Sternberg y Salter" que decían que la inteligencia es un "Comportamiento adaptativo dirigido a metas" nos damos cuenta que la relación entre la inteligencia real y la artificial se da por medio de la imitación o simulación, en otras palabras, la inteligencia artificial busca mediante modelos matemáticos emular la inteligencia real.

**(2) Explique cuál es el objetivo del uso de optimizadores en redes neuronales
(propagación hacia atrás y gradiente descendente).**

Tal y como lo dice su nombre, el sentido de los optimizadores es mejorar el proceso de entrenamiento de las redes neuronales, ayudando a que estas tengan una convergencia más eficiente, es decir, permitir que la red ajuste sus parámetros de manera efectiva para acelerar la convergencia hacia una solución adecuada. También son importantes para que las redes neuronales tengan un aprendizaje efectivo, en otras palabras, gracias a los ajustes que hace la red en sus pesos y sesgos esta aprende a capturar caraacterísticas relebantes de los datos y por consiguiente minimiza los errores.

##**SECCIÓN 2**

### Datos y configuraciones generales
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from time import time
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.layers import Convolution2D, MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.utils import to_categorical

(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()

input_size = 784
batch_size = 50
epochs = 200

"""###FULLY CONNECTED LAYER"""

X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)

classes = 10
Y_train = to_categorical(Y_train, classes)
Y_test = to_categorical(Y_test, classes)

modelFully = Sequential([
    Dense(1024, input_dim=input_size),
    Activation('relu'),
    Dense(512),
    Activation('sigmoid'),
    Dense(256),
    Activation('relu'),
    Dense(128),
    Activation('sigmoid'),
    Dense(64),
    Activation('relu'),
    Dense(classes),
    Activation('softmax')
])

modelFully.summary()

modelFully.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='sgd')

historyFullyConnected = modelFully.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, Y_test), verbose=1)

modelFully.save('FullyConnect1.h5')

"""###CONVOLUTIONAL LAYER"""

(X_trainConv, Y_trainConv), (X_testConv, Y_testConv) = fashion_mnist.load_data()

X_trainConv = X_train.reshape(60000, 28, 28, 1)
X_testConv = X_test.reshape(10000, 28, 28, 1)

classes = 10

Y_trainConv = to_categorical(Y_trainConv, classes)
Y_testConv = to_categorical(Y_testConv, classes)

modelConv = Sequential([

  Convolution2D(filters=28 ,kernel_size=(3, 3),input_shape=(28, 28, 1)),
  Activation('relu'),

  Convolution2D(filters=28, kernel_size=(3, 3)),
  Activation('sigmoid'),
  MaxPooling2D(pool_size=(2, 2)),

  Convolution2D(filters=28, kernel_size=(3, 3)),
  Activation('relu'),
  MaxPooling2D(pool_size=(2, 2)),
  Flatten(),

  Dense(128),
  Activation('relu'),
  Dense(64),
  Activation('sigmoid'),
  Dense(32),
  Activation('relu'),
  Dense(classes),
  Activation('softmax')
  ])

modelConv.summary()

modelConv.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='sgd')

historyConvModel = modelConv.fit(X_trainConv, Y_trainConv, batch_size=batch_size, epochs=epochs, validation_data=(X_testConv,Y_testConv), verbose=1)

modelConv.save('Convolutional1.h5')

"""##**PREDICCIONES**

#### Fully Connected
"""

loadedFully = tf.keras.models.load_model('/content/FullyConnect1.h5')
FullyPredicts = loadedFully.predict(X_test)

score = loadedFully.evaluate(X_test, Y_test, verbose=1)
print('Test accuracy:', score[1]*100, '%')

mse = tf.keras.losses.mean_squared_error(Y_test, FullyPredicts)
mse_average = tf.reduce_mean(mse)
print("Error Medio Cuadrático (MSE) promedio:", mse_average.numpy()*100, '%')

"""####Convolutional"""

loadedConv = tf.keras.models.load_model('/content/Convolutional1.h5')
ConvPredicts = loadedConv.predict(X_testConv)

score = loadedConv.evaluate(X_testConv, Y_testConv, verbose=1)
print('Test accuracy:', score[1]*100, '%')

mse = tf.keras.losses.mean_squared_error(Y_testConv, ConvPredicts)
mse_average = tf.reduce_mean(mse)
print("Error Medio Cuadrático (MSE) promedio:", mse_average.numpy()*100 , '%')

"""###**GRAFICOS**"""

import matplotlib.pyplot as plt

def plot_loss_acc(history):
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    epochs = range(len(train_loss))

    plt.figure(figsize=(8,6))
    plt.plot(epochs, train_loss, color='b', label='Train')
    plt.plot(epochs, val_loss, color='r', label='Validation')
    plt.legend()
    plt.title('Model Loss')


    plt.figure(figsize=(8,6))
    plt.plot(epochs, acc, color='b', label='Train')
    plt.plot(epochs, val_acc, color='r', label='Validation')
    plt.legend()
    plt.title('Model Accuracy')

    plt.show()

"""####Fully Connected"""

plot_loss_acc(historyFullyConnected)

"""####Convolutional layer"""

plot_loss_acc(historyConvModel)

"""####Imagenes del dataset"""

class_names = [
    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'
]
(train_images, train_labels), (_, _) = tf.keras.datasets.fashion_mnist.load_data()
def plot_images(images, labels, class_names):
    plt.figure(figsize=(10, 10))
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i], cmap=plt.cm.binary)
        plt.xlabel(class_names[labels[i]])

plot_images(train_images, train_labels, class_names)
plt.show()

"""#**PREGUNTAS**

###**¿Cuál fue el mejor modelo?**

Según las métricas obtenidas podemos decir que el mejor modelo fue convolucional

###**¿Por qué consideras que el modelo implementado fue el mejor?**

Porque en comapración al modelo Fully Connected este nos demuestra que apensar de tener un precisión muy alta con los datos de entrenamiento, también tiene buen desempeño con los datos de testeo, lo cual nos indica que le modelo no se acostumbro a las caracteristicas especificas de los datos de entrenamiento, esto lo podemos ver reflejado en el error medio cuadrático de las predicciones hechas con ambos modelos, donde el Fully connected tuvo un aproximado de 1.721% y el modelo convolucional obtuvo uno de 1.434%

###**¿Consideras que los resultados obtenidos pueden ser mejorados? ¿Como?**

Si, porque como podemos observar en las graficas, los parámetros de ambos modelos aún se pueden pulir para que la diferencia de comportamientos de los modelos seán más cercanos entre los datos de entrenamiento y los de testeo, algunas opciones de modificación para ello pueden ser ajustar los porcentajes de datos, probar con cambios en el tamaño del batch o inlcuso aplicar el principio de AlexNet y verificar si al tener una red Neuronal más profunda logramos mejores resultados.

###**¿Cuáles son los principales inconvenientes a la hora de implementar modelos de IA con bases de datos de imágenes?**

Uno de los principales inconvenientes a la hora de trabajar modelos de IA con bases de datos de imágenes es la calidad de las mismas, tanto desde el etiquetado de las imagenes, las cantidades o directamente la resolución de las imágenes porque en ocasiones tienen dimensiones demasiado grandes lo que representa costos computacionales muy elevados. La variedad también suele ser un obstaculo bastante significativo pues nos podemos encontrar en una misma base de datos que no hay cantidades iguales de imagenes de un mismo objeto lo cual puede influir significativamente en el aprendizaje de la IA. Por otro lado también debemos tener en cuenta que los temas de provacidad y derechos de imágen son un limitante significativo en cuanto a las bases de datos que involucran personas.
"""